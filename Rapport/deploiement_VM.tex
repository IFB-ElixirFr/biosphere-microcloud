\chapter{Installation et déploiement des VM} \label{chap:deploiement_VM}

Dans ce chapitre, on ne fait pas toujours la distinction entre un composant (au sens de composant SlipStream)
et une VM (qui est un composant déployé).
Il faut toutefois rappeler qu'un composant peut être déployé plusieurs fois.

De la même manière, nous utilisons le mot \frquote{composant} pour parler d'un composant SlipStream
et d'un composant (logiciel) de MicroScope.
Le contexte devrait permettre d'éviter toute ambiguïté.

\section{Vue d'ensemble}

Il y 5 composants Nuvla et
1 VM permanente déployée sur le cloud \cloudInstance{ifb-prabi-girofle}.
Le code des composants Nuvla est dans le dépôt \project{biosphere-microcloud}.
Il y a un dossier par composant et un fichier \filename{README.md} dans chaque dossier qui explique des détails.
Les principes du déploiement sont décrits sur le wiki du projet (voir la page
\href{https://intranet.genoscope.cns.fr/agc/redmine/projects/microcloud/wiki/Principes_de_fonctionnement_du_cloud_IFB}
{[[Principes de fonctionnement du cloud IFB]]}
en particulier la section
\href{https://intranet.genoscope.cns.fr/agc/redmine/projects/microcloud/wiki/Principes_de_fonctionnement_du_cloud_IFB#Utilisation-de-deacutepocircts-git-pour-le-code}
{[[Utilisation de dépôts git pour le code]]}
).

Les sections suivantes présentent quelques détails sur les VM:
\begin{itemize}
    \item \SScomponent{frontend} voir section \ref{frontend}
    \item \SScomponent{mysql} voir section \ref{mysql}
    \item VM permanente voir section \ref{VMpermanente}
    \item \SScomponent{master} et \SScomponent{slave} voir section \ref{master&slave}
    \item \SScomponent{nfsserver} voir section \ref{nfsserver}
\end{itemize}
Comme expliqué, le code de chaque composant est dans un dossier éponyme dans le dépôt biosphere-microcloud.
Dans les sections sus-citées, le nom des scripts est relatif à ce dossier (sauf mention contraire).

\section{Composant \SScomponent{frontend}}\label{frontend}

La composant \SScomponent{frontend} permet de déployer la partie web de la plateforme MicroScope.
L'image de base est une image CentOS 7.

\subsection{Configuration générale}

Le script \script{04\_Deployment.sh} réalise un certain nombre d'opérations avant l'installation de MicroScope:
\begin{itemize}
    \item Configuration de \component{Apache} (génération d'un certificat SSL)
          et \component{PHP} (variables \variable{memory\_limit}, \variable{max\_execution\_time},  \variable{max\_input\_time})
          nécessaires pour MicroScope.
    \item Configuration d'un alias du composant \SScomponent{mysql} sous le nom \hostname{mysqlagcdb.genoscope.cns.fr}
          (ceci a été fait pour ne pas avoir à modifier la configuration de MicroScope).
    \item Configuration de \component{phpMyAdmin}.
    \item Montage du répertoire partagé.
\end{itemize}

\begin{warningbox}
    La VM \SScomponent{frontend} utilise un client mariaDB (et non pas MySQL du fait de conflits existants entre le dépôt remi-php71 installé et le dépôt IUS qui fournit le client MySQL).
\end{warningbox}

\subsection{Installation de MicroScope}

Le script \script{install\_microscope.sh} réalise l'installation de MicroScope:
\begin{itemize}
    \item Création de l'utilisateur MySQL \user{agc} (avec le mot de passe habituel).
    \item Création des bases de données nécessaires à l'instance et copie des données minimales.
          Les tables nécessaires à l'installation de MicroScope (hors bases des banques qui sont dans la VM permanente)
          sont listées dans la page wiki \href{https://intranet.genoscope.cns.fr/agc/redmine/projects/microcloud/wiki/Tables_necessaires_a_installation}{[[Tables nécessaires à l'installation]]}.
    \item Copie du code web dans le répertoire \variable{DOCUMENT\_ROOT} de la VM (\path{/var/www/html/})
          et des scripts dans \path{/var/www/binphp/} (variable \variable{BIN} dans la configuration de MicroScope).
\end{itemize}

\subsection{Import des données de l'organisme \theOrg{}}

Les données de l'organisme \theOid{} sont copiées avec le script \script{import\_Oid.sh}.

\begin{warningbox}
    Si lors de la connexion au serveur web le message d'erreur \textbf{256} s'affiche, cela signifie qu'il n'y a pas d'organisme en base.
    De ce fait, la plupart des onglets ainsi que le formulaire d'authentification sont inaccessibles.
    Ceci ne doit pas arriver si le déploiement se passe bien mais en cas de modifications des scripts cela peut arriver.
\end{warningbox}

\subsection{Création des liens FEDERATED}

Les liens FEDERATED vers la VM permanente (qui permettent d'accéder aux base représentant les banques) sont créés
dans le script \script{create\_federated\_links.sh}.
Le script crée automatique un lien avec toutes les BD présentes sur la VM permanente sauf les BD système (\DB{mysql}, \DB{information\_schema}, \DB{performance\_schema}, \DB{sys})
et celles liées à \component{phpMyAdmin}.

\section{Composant \SScomponent{mysql}}\label{mysql}

Le composant \SScomponent{mysql} est le serveur de base de données de l'instance.
L'image de base est une image CentOS 7.

Comme cette VM se trouve sur le réseau privé, il faut passer par la VM frontend pour se connecter à la VM mysql:
\begin{lstlisting}[style=bash]
ssh -A centos@${IP_mysql} -J centos@${IP_frontend}
\end{lstlisting}

\subsection{Installation de MySQL}

Le serveur MySQL est installé via Docker.
La seule subtilité est qu'il faut démarrer MySQL avec l'option \variable{-{}-federated} pour activer le moteur FEDERATED.

Si le serveur ne répond pas, il faut aller voir si le docker n'a pas planté (cela arrive pour des requêtes SQL trop gourmandes en RAM).
Pour relancer le docker :
\begin{lstlisting}[style=bash]
sudo su
docker ps -a
docker start ${ID_container}
\end{lstlisting}

\subsection{Installation des fonctions UDF de MicroScope}

Les fonctions UDF nécessaires à MicroScope (\component{lib\_mysqludf\_sequtils})
sont téléchargées depuis le Genoscope et
sont installées dans le docker.

\begin{warningbox}
    Contrairement à la plupart des autres composants, la version des fonctions UDF utilisée
    est fixée dans les scripts (\micUDFVersion) et n'est pas gérée par \micWEBdeployVer{} (voir~\autoref{chap:micwebdeploy}).
    Ceci est lié au fait que c'est le composant qui a été développé en premier
    et que \component{lib\_mysqludf\_sequtils} évolue peu.
\end{warningbox}

\section{VM permanente}\label{VMpermanente}

C'est une VM OpenStack (\hostname{umr5558-microcloud.univ-lyon1.fr}, IP 134.214.33.214) du cloud \cloudInstance{ifb-prabi-girofle} disposant de 200 Go de stockage et 8 Go de RAM (elle est actuellement sous-dimensionnée par rapport à nos besoins).
L'image de base est une image Debian 9.8.

La machine permanente n'est accessible depuis l'extérieur qu'en SSH donc nous ne pouvons pas y accéder en MySQL (port 3306) depuis un autre cloud.
La connexion se fait avec des clés SSH (seules les clés SSH de Mathieu Dubois ont été importés).

La machine a été installée avec Sylvain Bonneval.
La procédure d'installation est dans le fichier \filename{Installation.md} du répertoire \path{/root}.
Logiciels installés : serveur MySQL (il n'est pas nécessaire de démarrer le serveur avec l'option \variable{-{}-federated}), rsync, phpMyAdmin (installé mais non configuré).

Pour se connecter :
\begin{lstlisting}[style=bash]
ssh root@134.214.33.214
\end{lstlisting}

\subsection{Création des BD et copie des données}

Le serveur MySQL contient les schémas des banques et les données nécessaires pour les tests (nous avons testés les onglets \textbf{Genome Browser}, \textbf{Identical Gene Names}):
\begin{itemize}
    \item les schéma des bases \DB{ANTISMASHDB}, \DB{CARDDB}, \DB{COGDB}, \DB{EGGNOGDB}, \DB{ENZYMEDB}, \DB{ESSDB}, \DB{FIGFAMDB}, \DB{INTERPRODATADB},
          \DB{KEGGDB}, \DB{RHEADB}, \DB{TAXONOMYDB}, \DB{TIGRFAMDB}, \DB{UNIFIREDB}, \DB{UNIPROTKBDB}, \DB{VIRULENCEDB}, \DB{microcyc}\footnote{Bien que cette base contienne les résultats d'une instance elle est nécessaire à l'heure actuelle car elle contient aussi les données de référence.}
          et \DB{DBWORKFLOW}.
    \item une partie des données de la base \DB{UNIPROTKBDB} pour les tests.
    \item les données de \DB{DBWORKFLOW}.
    \item les données de l'organisme \theOrg{} (taxon id \theTaxID{}) dans la base \DB{TAXONOMYDB}.
\end{itemize}

Le script \script{microscopeDBschema.py} du module MicroCloud a été utilisé pour créer les schémas.

Pour se connecter au serveur MySQL :
\begin{lstlisting}[style=bash]
mysql -p{MOT_DE_PASSE_SERVEUR_MYSQL_VM_PERMANENTE}
\end{lstlisting}

\section{Compoants \SScomponent{master} et \SScomponent{slave}} \label{master&slave}

Ces 2 composants permettent de créer un cluster basé sur SLURM sur lequel \component{jbpmmicroscope} soumet les jobs:
\SScomponent{master} est la frontale et \SScomponent{slave} est un nœud de calcul.
\begin{itemize}
    \item Les composants SlipStream \SScomponent{master} et \SScomponent{slave} ont été copiés depuis
    l'appliance \href{https://nuv.la/module/ifb/devzone/jlorenzo/cluster/Slurm_Cluster_ubuntu18}{\SScomponent{Slurm\_Cluster\_ubuntu18}}
          (crée par Jonathan Lorenzo et Bryan Brancotte pour l'IFB).
          L'image de base de ces composants est une image Ubuntu 18.04.
    \item Le code d'installation de ces composants clone le dépôt \project{biosphere-commons} pour installer le cluster
          comme cela est fait dans les composants originaux
          et installe les parties spécifiques à MicroCloud (voir plus bas).
\end{itemize}
Le composant \component{slave} peut être instancié plusieurs fois (2 par défaut).
Ceci est choisi lors du déploiement (voir~\autoref{chap:deploiement}).

Le code utilise les fonctionnalités avancées de SlipStream pour permettre un redimensionnement à chaud
(voir \href{https://intranet.genoscope.cns.fr/agc/redmine/projects/microcloud/wiki/Principes_de_fonctionnement_du_cloud_IFB}{[[Principes de fonctionnement du cloud IFB]]}).
Ainsi, il y a des scripts pour les étapes \script{06\_on\_VM\_add.sh} et \script{07\_on\_VM\_remove.sh} (dans les 2 composants).
Cependant, nous ne sommes pas sûrs que le redimensionnement à chaud fonctionne.

\subsection{Généralités}

Les composants \SScomponent{master} et \SScomponent{slave} montent le dossier partagé \path{/env/} (voir~\autoref{nfsserver}).
Le composant \SScomponent{slave} est peu modifié par rapport aux scripts de J. Lorenzo et B. Brancotte.
En revanche, le composant \SScomponent{master} est plus complexe car
c'est sur lui qu'on installe \component{jbpmmicroscope} (et donc \component{Tomcat})
et les logiciels partagés entre la frontale et les nœuds de calcul.

Le composant \SScomponent{master} crée un alias du composant \SScomponent{mysql} sous le nom \hostname{mysqlagcdb.genoscope.cns.fr}.
Ceci est utilisé pour l'installation de \component{jbpmmicroscope} (voir~\autoref{sec:installation_jbpmmicroscope}).

\subsection{Installation de \component{jbpmmicroscope}}

Le script \script{install\_jbpm.sh} installe :
\begin{enumerate}
    \item Le serveur d'application \href{http://mirrors.ircam.fr/pub/apache/tomcat/tomcat-9/v9.0.31/bin/apache-tomcat-9.0.31.tar.gz}{\component{Tomcat} version 9.0.31} dans \path{/env/cns/proj/agc/tools/COMMON/JBPMmicroscope/tomcat}.
    Les identifiants tomcat sont les suivants:
    \begin{description}
        \item[login:] \variable{tomcat}
        \item[password:] \variable{tomcat}
    \end{description}
    Ces identifiants peuvent être modifiés dans le composant master, ils correspondent aux paramètres \variable{tomcat\_user} et \variable{tomcat\_password}.
    Pour accéder à Tomcat : http://\$IP\_master.

    \item \component{jbpmmicroscope} est installé à partir des fichiers téléchargés depuis le Genoscope
    dans path{/env/cns/proj/agc/tools/COMMON/JBPMmicroscope/jbpmmicroscope}.
    Pour les logins et les mots de passe, voir~\autoref{sec:installation_jbpmmicroscope}.
\end{enumerate}

\subsection{Installation de \component{pegasus-mpi-cluster}}

\component{bagsub} s'appuie sur \component{pegasus-mpi-cluster}.
Nous avons installé (\href{https://github.com/pegasus-isi/pegasus/archive/4.9.2.zip}{\component{pegasus-mpi-cluster} version 4.9.2}).
Le code téléchargé ne compile pas à cause car la librairie \component{libnuma} n'est pas installée.
Comme celle-ci n'est pas nécessaire pour les architectures non-NUMA, nous modifions les fichiers \filename{Makefile}
pour supprimer la dépendance à \component{libnuma}.

\subsection{Installation des modules (dont \component{bagsub})}

Chaque workflow fait appel à différents modules qu'il faut installer au préalable sur la VM master.
Le script \script{import\_modules.sh} permet d'importer les archives des différents modules nécessaires au fonctionnement des workflows (WF DIRECTON principalement).

Le script \script{import\_modules.sh} fait quelques modifications du module \component{bagsub} afin qu'il puisse être utilisé dans l'environnement fourni par la VM \SScomponent{master}:
on supprime l'option ulimit qui ne fonctionne pas en \component{dash} (le shell sous Ubuntu 18.04)
et ajoute le paramètre MCA \variable{btl\_tcp\_if\_exclude} pour exclure les interfaces réseaux \hostname{docker0} et \hostname{lo} qui peuvent engendrer des conflits lors du lancement de jobs SLURM.


\section{Composant \SScomponent{nfsserver}} \label{nfsserver}

Le composant \SScomponent{nfsserver} a été créé en s'inspirant du travail de Stéphane Delmotte.
Il permet de fournir un dossier partagé entre les différentes VM.
L'image de base est une image CentOS 7.

Le répertoire \path{/var/nfsshare} du serveur NFS est monté dans le répertoire \path{/env}
des composants \SScomponent{frontend}, \SScomponent{backend}, \SScomponent{master} et \SScomponent{slave}.
Le répertoire \path{/env/} se veut similaire au répertoire de même nom sur le cluster \hostname{etna}.

Les fonctions permettant de monter le répertoire partagé sur les autres composants sont dans le fichier \script{lib.sh}
(qui est à la racine du dépôt \project{biosphere-microcloud} car il est utilisé par tous les composants).
Le code a été copié depuis le dépôt \project{biosphere-commons}.

% Faire un tableau récapitulatf ?